"""
LinkedIn Fraud Detector API - Railway Î∞∞Ìè¨Ïö© (BERT Ï†úÍ±∞)
‚úÖ BERT ÏôÑÏ†Ñ Ï†úÍ±∞ (Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî)
‚úÖ v17 Î™®Îç∏ ÌååÏùºÎ™Ö ÏûêÎèô Ìò∏Ìôò
‚úÖ Railway ÏµúÏ†ÅÌôî ÏôÑÎ£å
"""

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import sys
import os
import logging

# ========================================
# numpy Ìò∏ÌôòÏÑ± Ìå®Ïπò (ÌïÑÏàò!)
# ========================================
import numpy as np
if not hasattr(np, '_core'):
    import numpy.core
    sys.modules['numpy._core'] = numpy.core
    np._core = numpy.core

import dill
import pandas as pd

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

app = FastAPI(
    title="LinkedIn Fraud Detector API",
    version="3.1.0",
    description="AI-powered job posting fraud detection (Optimized for Railway)"
)

# ========================================
# CORS ÏÑ§Ï†ï
# ========================================
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
    max_age=3600,
)

# ========================================
# Ï†ÑÏó≠ Î≥ÄÏàò
# ========================================
extractor = None
tfidf = None
embedder = None
model = None
ensemble_models = None
use_ensemble = False
threshold = 0.5
feature_names = []
model_loaded = False
model_version = "unknown"

# ========================================
# Î™®Îç∏ Î°úÎìú Ìï®Ïàò
# ========================================
def load_model():
    """Î™®Îç∏ Î°úÎìú (v17 Ìò∏Ìôò)"""
    global extractor, tfidf, embedder, model, ensemble_models
    global use_ensemble, threshold, feature_names, model_loaded, model_version

    logger.info("üîÑ Î™®Îç∏ Î°úÎî© ÏãúÏûë...")

    try:
        # üî• Î™®Îç∏ ÌååÏùº ÏûêÎèô ÌÉêÏÉâ (v17 Ïö∞ÏÑ†)
        possible_files = [
            'model_v17_lightweight.pkl',
            'production_model_v17.pkl',
            'model_v17.pkl',
            'production_model_v13_enhanced_rules.pkl',
            'fraud_detection_render_v2.pkl',
            'model.pkl'
        ]

        pkl_path = None
        for filename in possible_files:
            test_path = os.path.join(current_dir, filename)
            if os.path.exists(test_path):
                pkl_path = test_path
                logger.info(f"‚úÖ Î™®Îç∏ Î∞úÍ≤¨: {filename}")
                break

        if pkl_path is None:
            logger.error(f"‚ùå Î™®Îç∏ ÌååÏùº ÏóÜÏùå. ÌôïÏù∏ ÌïÑÏöî: {possible_files}")
            return False

        logger.info(f"üìÇ Î°úÎî© Ï§ë: {pkl_path}")

        # Î™®Îç∏ Î°úÎìú
        with open(pkl_path, 'rb') as f:
            artifacts = dill.load(f)

        # Î≤ÑÏ†Ñ ÌôïÏù∏
        model_version = artifacts.get('version', 'unknown')
        logger.info(f"  ‚úì Î™®Îç∏ Î≤ÑÏ†Ñ: {model_version}")

        # Feature Extractor (Ïó¨Îü¨ ÌÇ§ ÏãúÎèÑ)
        for key in ['feature_extractor', 'extractor']:
            if key in artifacts:
                extractor = artifacts[key]
                logger.info(f"  ‚úì Feature Extractor: {type(extractor).__name__}")
                break

        if extractor is None:
            logger.error("  ‚ùå Feature Extractor ÏóÜÏùå")
            return False

        # TF-IDF (ÏÑ†ÌÉùÏ†Å)
        if 'tfidf' in artifacts:
            tfidf = artifacts['tfidf']
            logger.info("  ‚úì TF-IDF")

        # FastText Embedder (v17 Ï†ÑÏö©)
        if 'embedder' in artifacts:
            embedder = artifacts['embedder']
            logger.info("  ‚úì FastText Embedder")

        # Feature names
        if 'feature_names' in artifacts:
            feature_names = artifacts['feature_names']
            logger.info(f"  ‚úì Features: {len(feature_names)}Í∞ú")

        # Threshold
        if 'threshold' in artifacts:
            threshold = artifacts['threshold']
            logger.info(f"  ‚úì Threshold: {threshold:.3f}")

        # Model (Single or Ensemble)
        if 'use_ensemble' in artifacts and artifacts['use_ensemble']:
            use_ensemble = True
            ensemble_models = artifacts.get('ensemble_models', {})
            logger.info(f"  ‚úì Ensemble Î™®Îìú: {len(ensemble_models)}Í∞ú Î™®Îç∏")
        else:
            model = artifacts.get('model')
            model_name = artifacts.get('model_name', 'Unknown')
            logger.info(f"  ‚úì Single Î™®Îç∏: {model_name}")

        if model is None and (not use_ensemble or not ensemble_models):
            logger.error("  ‚ùå Î™®Îç∏ ÏóÜÏùå")
            return False

        model_loaded = True
        logger.info("‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!")
        return True

    except Exception as e:
        logger.error(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
        import traceback
        traceback.print_exc()
        model_loaded = False
        return False

# ÏÑúÎ≤Ñ ÏãúÏûë Ïãú Î™®Îç∏ Î°úÎìú
@app.on_event("startup")
async def startup_event():
    """ÏÑúÎ≤Ñ ÏãúÏûë Ïãú Î™®Îç∏ Î°úÎìú"""
    logger.info("üöÄ ÏÑúÎ≤Ñ ÏãúÏûë...")
    success = load_model()
    if not success:
        logger.warning("‚ö†Ô∏è  Î™®Îç∏ ÏóÜÏù¥ ÏÑúÎ≤Ñ ÏãúÏûë (Ìó¨Ïä§Ï≤¥ÌÅ¨Îßå Í∞ÄÎä•)")
    else:
        logger.info("‚úÖ API Ï§ÄÎπÑ ÏôÑÎ£å!")

# ========================================
# Pydantic Î™®Îç∏
# ========================================
class JobPosting(BaseModel):
    title: str = ""
    description: str = ""
    company_profile: str = ""
    salary_range: str = ""
    requirements: str = ""
    benefits: str = ""
    has_company_logo: int = 0
    telecommuting: int = 0
    has_questions: int = 0
    industry: str = ""
    function: str = ""
    location: str = ""
    department: str = ""
    employment_type: str = ""
    required_experience: str = ""
    required_education: str = ""

# ========================================
# API ÏóîÎìúÌè¨Ïù∏Ìä∏
# ========================================
@app.get("/")
async def root():
    """Î£®Ìä∏ ÏóîÎìúÌè¨Ïù∏Ìä∏"""
    return {
        "message": "üîç LinkedIn Fraud Detector API",
        "version": "3.1.0",
        "status": "online" if model_loaded else "model not loaded",
        "model": {
            "loaded": model_loaded,
            "version": model_version,
            "extractor": type(extractor).__name__ if extractor else None,
            "features": len(feature_names) if feature_names else 0,
            "mode": "ensemble" if use_ensemble else "single",
            "threshold": float(threshold),
            "has_tfidf": tfidf is not None,
            "has_embedder": embedder is not None
        },
        "endpoints": {
            "health": "GET /health",
            "analyze": "POST /analyze",
            "reload": "POST /reload"
        }
    }

@app.head("/")
async def head_root():
    return JSONResponse(content={}, status_code=200)

@app.get("/health")
async def health():
    """Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
    return {
        "status": "healthy" if model_loaded else "model not loaded",
        "model_loaded": model_loaded,
        "model_version": model_version,
        "extractor": type(extractor).__name__ if extractor else None,
        "tfidf_loaded": tfidf is not None,
        "embedder_loaded": embedder is not None,
        "mode": "ensemble" if use_ensemble else "single",
        "features": len(feature_names) if feature_names else 0,
    }

@app.head("/health")
async def head_health():
    return JSONResponse(content={}, status_code=200)

@app.post("/reload")
async def reload_model():
    """Î™®Îç∏ Ïû¨Î°úÎìú (Í¥ÄÎ¶¨ÏûêÏö©)"""
    logger.info("üîÑ Î™®Îç∏ Ïû¨Î°úÎìú ÏöîÏ≤≠...")
    success = load_model()
    return {
        "status": "success" if success else "failed",
        "model_loaded": model_loaded,
        "model_version": model_version
    }

@app.options("/analyze")
async def options_analyze():
    return JSONResponse(
        content={"status": "ok"},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.head("/analyze")
async def head_analyze():
    return JSONResponse(content={}, status_code=200)

@app.post("/analyze")
async def analyze_job(job: JobPosting):
    """Ï±ÑÏö©Í≥µÍ≥† ÏÇ¨Í∏∞ ÌÉêÏßÄ Î∂ÑÏÑù"""
    logger.info(f"üì® Î∂ÑÏÑù ÏöîÏ≤≠: {job.title[:50] if job.title else 'No title'}...")

    if not model_loaded or extractor is None:
        logger.error("‚ùå Î™®Îç∏ ÎØ∏Î°úÎìú")
        raise HTTPException(
            status_code=503,
            detail="Model not loaded. Please contact administrator."
        )

    try:
        # ÏûÖÎ†• Í≤ÄÏ¶ù
        if not job.title and not job.description:
            raise HTTPException(
                status_code=400,
                detail="Title or description required"
            )

        # DataFrame ÏÉùÏÑ±
        df = pd.DataFrame([job.dict()])
        logger.info(f"  ‚úì Input validated")

        # ========================================
        # Feature Ï∂îÏ∂ú (v17 Ìò∏Ìôò)
        # ========================================

        # 1. Domain Features
        if hasattr(extractor, 'transform'):
            X_domain = extractor.transform(df)
        elif hasattr(extractor, 'extract_all_features'):
            features_dict = extractor.extract_all_features(df.iloc[0].to_dict())
            X_domain = pd.DataFrame([features_dict])
        else:
            raise HTTPException(status_code=500, detail="Incompatible feature extractor")

        logger.info(f"  ‚úì Domain features: {X_domain.shape}")

        # 2. TF-IDF (ÏÑ†ÌÉùÏ†Å)
        X_tfidf_df = pd.DataFrame()
        if tfidf is not None:
            try:
                texts = (df['title'].fillna('') + ' ' +
                        df['description'].fillna('') + ' ' +
                        df['requirements'].fillna('')).tolist()
                X_tfidf = tfidf.transform(texts)
                X_tfidf_df = pd.DataFrame(
                    X_tfidf.toarray(),
                    columns=[f'tfidf_{i}' for i in range(X_tfidf.shape[1])],
                    index=X_domain.index
                )
                logger.info(f"  ‚úì TF-IDF: {X_tfidf_df.shape}")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è  TF-IDF Ïä§ÌÇµ: {e}")

        # 3. FastText Embedder (v17 Ï†ÑÏö©)
        X_embedder_df = pd.DataFrame()
        if embedder is not None:
            try:
                texts = (df['title'].fillna('') + ' ' +
                        df['description'].fillna('')).tolist()

                if hasattr(embedder, 'transform'):
                    embeddings = embedder.transform(texts)
                elif hasattr(embedder, 'get_embedding'):
                    embeddings = np.array([embedder.get_embedding(t) for t in texts])
                else:
                    embeddings = None

                if embeddings is not None:
                    X_embedder_df = pd.DataFrame(
                        embeddings,
                        columns=[f'embed_{i}' for i in range(embeddings.shape[1])],
                        index=X_domain.index
                    )
                    logger.info(f"  ‚úì Embedder: {X_embedder_df.shape}")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è  Embedder Ïä§ÌÇµ: {e}")

        # ========================================
        # ÌäπÏÑ± Í≤∞Ìï©
        # ========================================
        dfs_to_concat = [X_domain]
        if not X_tfidf_df.empty:
            dfs_to_concat.append(X_tfidf_df)
        if not X_embedder_df.empty:
            dfs_to_concat.append(X_embedder_df)

        X_combined = pd.concat(dfs_to_concat, axis=1)
        logger.info(f"  ‚úì Combined features: {X_combined.shape}")

        # ========================================
        # ÏòàÏ∏°
        # ========================================
        if use_ensemble and ensemble_models:
            # Ensemble Î™®Îìú
            probas = []
            for name, m in ensemble_models.items():
                try:
                    proba = m.predict_proba(X_combined)[0, 1]
                    probas.append(proba)
                    logger.info(f"    ‚Ä¢ {name}: {proba:.4f}")
                except Exception as e:
                    logger.warning(f"    ‚ö†Ô∏è  {name} Ïã§Ìå®: {e}")

            if not probas:
                raise HTTPException(status_code=500, detail="All ensemble models failed")

            balanced_proba = float(np.mean(probas))
            logger.info(f"  ‚úì Ensemble ÌèâÍ∑†: {balanced_proba:.4f}")
        else:
            # Single Î™®Îìú
            if model is None:
                raise HTTPException(status_code=503, detail="Model not available")

            balanced_proba = float(model.predict_proba(X_combined)[0, 1])
            logger.info(f"  ‚úì Probability: {balanced_proba:.4f}")

        # ========================================
        # ÌåêÏ†ï
        # ========================================
        prediction = 1 if balanced_proba >= threshold else 0

        if prediction == 1:
            if balanced_proba > 0.80:
                action = 'BLOCK'
                reason = 'Very high fraud probability - Immediate block recommended'
                risk_level = 'CRITICAL'
            else:
                action = 'REVIEW'
                reason = 'High risk - Manual review strongly recommended'
                risk_level = 'HIGH'
        else:
            if balanced_proba > 0.30:
                action = 'REVIEW'
                reason = 'Medium risk - Consider manual review'
                risk_level = 'MEDIUM'
            else:
                action = 'PASS'
                reason = 'Appears to be a legitimate job posting'
                risk_level = 'LOW'

        result = {
            'action': action,
            'reason': reason,
            'risk_level': risk_level,
            'probability': balanced_proba,
            'prediction': prediction,
            'threshold': float(threshold),
            'model_version': model_version
        }

        logger.info(f"  ‚úÖ Í≤∞Í≥º: {action} (prob={balanced_proba:.3f}, risk={risk_level})")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Î∂ÑÏÑù Ïã§Ìå®: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

# ========================================
# Catch-all handlers
# ========================================
@app.options("/{path:path}")
async def catch_all_options(path: str):
    return JSONResponse(
        content={"status": "ok"},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.head("/{path:path}")
async def catch_all_head(path: str):
    return JSONResponse(content={}, status_code=200)

# ========================================
# ÎØ∏Îì§Ïõ®Ïñ¥
# ========================================
@app.middleware("http")
async def log_requests(request: Request, call_next):
    logger.info(f"üì• {request.method} {request.url.path}")
    try:
        response = await call_next(request)
        logger.info(f"üì§ {response.status_code}")
        return response
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        raise

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"üöÄ ÏÑúÎ≤Ñ ÏãúÏûë: 0.0.0.0:{port}")
    uvicorn.run(app, host="0.0.0.0", port=port)